Jasne, oto plan dla agenta kodujÄ…cego oraz treÅ›Ä‡ merytoryczna do moduÅ‚u "Baza Wiedzy" dotyczÄ…ca K-Means Clustering.

Terminologia "wszystkie moÅ¼liwe rozwiÄ…zania" w kontekÅ›cie K-Means nie oznacza nieskoÅ„czonych moÅ¼liwoÅ›ci, ale odnosi siÄ™ do **metod znajdowania optymalnej liczby klastrÃ³w (K)** oraz **kluczowych technik przygotowania danych i inicjalizacji**, ktÃ³re sÄ… niezbÄ™dne, aby algorytm zadziaÅ‚aÅ‚ poprawnie. ModuÅ‚ skupi siÄ™ na dostarczeniu narzÄ™dzi do oceny rÃ³Å¼nych wartoÅ›ci 'K' (Metoda Åokcia, Analiza Sylwetkowa) oraz na prawidÅ‚owym uruchomieniu algorytmu.

-----

## ğŸš€ Plan dla Agenta: Budowa ModuÅ‚u K-Means Clustering

**Cel:** Stworzenie moduÅ‚u, ktÃ³ry pozwala na wgranie danych, przeskalowanie ich, analizÄ™ optymalnej liczby klastrÃ³w (K) przy uÅ¼yciu Metody Åokcia i Analizy Sylwetkowej, a nastÄ™pnie uruchomienie algorytmu K-Means i wizualizacjÄ™ wynikÃ³w.

### 1\. Struktura Pliku i Importy

Sugerowany plik: `clustering_module.py`.

**Wymagane biblioteki:**

```python
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.datasets import make_blobs # Do danych demo
import plotly.express as px
import plotly.graph_objects as go
```

### 2\. GÅ‚Ã³wna Funkcja ModuÅ‚u

```python
def run_clustering_module():
    st.title("ğŸ’  ModuÅ‚ Klastrowania K-Means")
    st.markdown("Grupuj dane i odkrywaj ukryte struktury za pomocÄ… algorytmu K-Means.")
    
    # Krok 1: Wprowadzenie Danych (z Demo)
    # Krok 2: Konfiguracja Modelu (WybÃ³r cech, Skalowanie)
    # Krok 3: Znajdowanie Optymalnego 'K' (Metody analityczne)
    # Krok 4: Uruchomienie Modelu i Wizualizacja
```

### 3\. Krok 1: Wprowadzenie Danych

Logika identyczna jak w module regresji, ale z nowymi danymi demo.

```python
    # === SEKCJA 1: Å¹RÃ“DÅO DANYCH ===
    st.sidebar.header("1. WprowadÅº Dane")
    data_source = st.sidebar.radio(
        "Wybierz ÅºrÃ³dÅ‚o danych:",
        ("UÅ¼yj danych demo (3 klastry)", "WprowadÅº wÅ‚asne dane")
    )
    
    df = None 

    def get_demo_data_clustering():
        # Generuj dane z wyraÅºnymi grupami
        X, y = make_blobs(n_samples=300, centers=3, n_features=2, 
                          cluster_std=1.0, random_state=42)
        demo_df = pd.DataFrame(X, columns=['Cecha_1', 'Cecha_2'])
        demo_df['Prawdziwy_Klaster'] = y # Do ewentualnej walidacji, choÄ‡ model tego nie widzi
        return demo_df

    if data_source == "UÅ¼yj danych demo (3 klastry)":
        df = get_demo_data_clustering()
    else:
        uploaded_file = st.sidebar.file_uploader("Wgraj plik CSV lub Excel", type=["csv", "xlsx"])
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
            except Exception as e:
                st.error(f"BÅ‚Ä…d podczas wczytywania pliku: {e}")

    if df is None or df.empty:
        st.info("ProszÄ™ wczytaÄ‡ dane lub wybraÄ‡ zestaw demo, aby rozpoczÄ…Ä‡ analizÄ™.")
        st.stop()

    st.subheader("PodglÄ…d Danych")
    st.dataframe(df.head())
```

### 4\. Krok 2: Konfiguracja Modelu

To jest kluczowy etap: wybÃ³r cech i **skalowanie**.

```python
    # === SEKCJA 2: KONFIGURACJA MODELU ===
    st.sidebar.header("2. Konfiguracja Modelu")
    
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    
    # UsuÅ„ kolumnÄ™ 'Prawdziwy_Klaster' z danych demo, jeÅ›li tam jest
    if 'Prawdziwy_Klaster' in numeric_cols:
        numeric_cols.remove('Prawdziwy_Klaster')

    if len(numeric_cols) == 0:
        st.warning("Twoje dane muszÄ… zawieraÄ‡ kolumny numeryczne.")
        st.stop()

    selected_features = st.sidebar.multiselect(
        "Wybierz cechy (zmienne) do klastrowania:",
        numeric_cols,
        default=numeric_cols[0:min(len(numeric_cols), 2)] # DomyÅ›lnie wybierz pierwsze dwie
    )

    if len(selected_features) == 0:
        st.warning("ProszÄ™ wybraÄ‡ co najmniej jednÄ… cechÄ™ do analizy.")
        st.stop()

    # --- Metoda: Standaryzacja Danych ---
    do_scale = st.sidebar.checkbox(
        "Standaryzuj dane (Zalecane!)", 
        value=True,
        help="K-Means jest wraÅ¼liwy na skalÄ™ danych (np. 'Wiek' vs 'Zarobki'). Standaryzacja (X-Å›rednia)/odch. std. sprawia, Å¼e wszystkie cechy majÄ… rÃ³wnÄ… wagÄ™."
    )
    
    # Przygotowanie danych do analizy
    X_raw = df[selected_features].dropna()
    
    if X_raw.empty:
        st.error("Wybrane kolumny nie zawierajÄ… danych po usuniÄ™ciu brakÃ³w (NaN).")
        st.stop()
        
    X_scaled = X_raw.copy()
    if do_scale:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X_raw)
    else:
        X_scaled = X_raw.values # UÅ¼yj .values, aby mieÄ‡ spÃ³jny typ (numpy array)
```

### 5\. Krok 3: Znajdowanie Optymalnego 'K' (Metody)

Tutaj implementujemy "wszystkie sposoby" oceny K.

```python
    # === SEKCJA 3: ZNAJDOWANIE OPTYMALNEGO 'K' ===
    st.header("Analiza Optymalnej Liczby KlastrÃ³w (K)")
    st.markdown("""
    Algorytm K-Means wymaga podania z gÃ³ry liczby klastrÃ³w (K). PoniÅ¼sze metody statystyczne pomogÄ… Ci dokonaÄ‡ Å›wiadomego wyboru.
    """)

    max_k = st.sidebar.slider("Maksymalna liczba 'K' do analizy:", 2, 15, 10)
    
    # Przechowywanie wynikÃ³w
    inertia_values = [] # Dla Metody Åokcia
    silhouette_values = [] # Dla Analizy Sylwetkowej
    k_range = range(2, max_k + 1)

    # UÅ¼yj paska postÄ™pu
    progress_bar = st.progress(0, text="Analizowanie K...")
    
    for i, k in enumerate(k_range):
        kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)
        kmeans.fit(X_scaled)
        
        # Metoda Åokcia
        inertia_values.append(kmeans.inertia_)
        
        # Metoda Sylwetkowa
        score = silhouette_score(X_scaled, kmeans.labels_)
        silhouette_values.append(score)
        
        progress_bar.progress((i + 1) / len(k_range), text=f"Analizowanie K={k}...")
    progress_bar.empty()


    # WyÅ›wietlanie wynikÃ³w w zakÅ‚adkach
    tab1, tab2 = st.tabs(["Metoda Åokcia (Inertia)", "Analiza Sylwetkowa (Silhouette Score)"])

    with tab1:
        st.subheader("Metoda Åokcia (Elbow Method)")
        fig_elbow = go.Figure()
        fig_elbow.add_trace(go.Scatter(x=list(k_range), y=inertia_values, mode='lines+markers'))
        fig_elbow.update_layout(
            title="Suma KwadratÃ³w OdlegÅ‚oÅ›ci wewnÄ…trz KlastrÃ³w (Inertia)",
            xaxis_title="Liczba KlastrÃ³w (K)",
            yaxis_title="Inertia (WCSS)"
        )
        st.plotly_chart(fig_elbow, use_container_width=True)
        st.markdown("**Jak interpretowaÄ‡:** Szukaj 'zaÅ‚amania' (Å‚okcia) na wykresie. Jest to punkt, w ktÃ³rym dodanie kolejnego klastra nie przynosi juÅ¼ znaczÄ…cej redukcji sumy bÅ‚Ä™dÃ³w. To sugeruje optymalne K.")

    with tab2:
        st.subheader("Analiza Sylwetkowa (Silhouette Analysis)")
        fig_silhouette = go.Figure()
        fig_silhouette.add_trace(go.Scatter(x=list(k_range), y=silhouette_values, mode='lines+markers'))
        fig_silhouette.update_layout(
            title="Åšredni WspÃ³Å‚czynnik Sylwetkowy",
            xaxis_title="Liczba KlastrÃ³w (K)",
            yaxis_title="Silhouette Score"
        )
        st.plotly_chart(fig_silhouette, use_container_width=True)
        st.markdown("**Jak interpretowaÄ‡:** Wynik bliski +1 oznacza, Å¼e klastry sÄ… gÄ™ste i dobrze odseparowane. Wynik bliski 0 oznacza nakÅ‚adanie siÄ™ klastrÃ³w. **Szukaj wartoÅ›ci 'K', ktÃ³ra daje najwyÅ¼szy (maksymalny) wynik.**")
```

### 6\. Krok 4: Uruchomienie Modelu i Wizualizacja

```python
    # === SEKCJA 4: WYNIKI KLASTROWANIA ===
    st.header("Uruchomienie Modelu K-Means")

    # WybÃ³r ostatecznego K przez uÅ¼ytkownika
    st.sidebar.header("3. Uruchom Model")
    final_k = st.sidebar.number_input(
        "Wybierz ostatecznÄ… liczbÄ™ klastrÃ³w (K):",
        min_value=2, 
        max_value=max_k, 
        value=silhouette_values.index(max(silhouette_values)) + 2, # Sugeruj K z najlepszym Silhouette
        help="Wybierz K na podstawie analizy z Metody Åokcia i Analizy Sylwetkowej."
    )

    # Uruchomienie finalnego modelu
    final_kmeans = KMeans(n_clusters=final_k, init='k-means++', n_init=10, random_state=42)
    final_kmeans.fit(X_scaled)
    cluster_labels = final_kmeans.labels_

    # Dodanie wynikÃ³w do oryginalnego DataFrame
    df_results = X_raw.copy()
    df_results['cluster'] = cluster_labels
    
    st.subheader(f"Wizualizacja KlastrÃ³w (K={final_k})")

    # Wizualizacja 2D/3D
    if len(selected_features) == 2:
        fig_clusters = px.scatter(
            df_results, 
            x=selected_features[0], 
            y=selected_features[1], 
            color='cluster',
            color_continuous_scale=px.colors.qualitative.Vivid,
            title="Wyniki Klastrowania (Dane Oryginalne)"
        )
        st.plotly_chart(fig_clusters, use_container_width=True)
    elif len(selected_features) == 3:
        st.info("Tworzysz klastry w 3D. MoÅ¼esz obracaÄ‡ poniÅ¼szy wykres.")
        fig_clusters = px.scatter_3d(
            df_results, 
            x=selected_features[0], 
            y=selected_features[1], 
            z=selected_features[2], 
            color='cluster',
            color_continuous_scale=px.colors.qualitative.Vivid,
            title="Wyniki Klastrowania (Dane Oryginalne)"
        )
        st.plotly_chart(fig_clusters, use_container_width=True)
    else:
        st.warning(f"Klastrowanie przeprowadzono na {len(selected_features)} cechach. Wizualizacja jest moÅ¼liwa tylko dla 2 lub 3 cech. PokazujÄ™ pierwsze dwie:")
        fig_clusters = px.scatter(
            df_results, 
            x=selected_features[0], 
            y=selected_features[1], 
            color='cluster',
            color_continuous_scale=px.colors.qualitative.Vivid,
            title=f"Wizualizacja 2D (uÅ¼yto {len(selected_features)} cech)"
        )
        st.plotly_chart(fig_clusters, use_container_width=True)

    # === SEKCJA 5: INTERPRETACJA ===
    st.header("Charakterystyka Odkrytych KlastrÃ³w")
    st.markdown("PoniÅ¼sza tabela pokazuje **Å›rednie wartoÅ›ci** kaÅ¼dej cechy dla kaÅ¼dego klastra. Pomaga to zrozumieÄ‡ i 'nazwaÄ‡' (stworzyÄ‡ persony) dla kaÅ¼dej z grup.")
    
    cluster_summary = df_results.groupby('cluster')[selected_features].mean()
    st.dataframe(cluster_summary.style.format("{:.2f}"))

    st.subheader("PodglÄ…d Danych z Przypisanymi Klastrami")
    st.dataframe(df_results.head())
```

-----

## ğŸ“ Integracja z moduÅ‚em "Baza Wiedzy"

Agencie, poniÅ¼szy tekst w formacie Markdown powinien zostaÄ‡ dodany jako nowa strona lub sekcja w istniejÄ…cym module "Baza Wiedzy".

**Instrukcja dla agenta:**

1.  Zlokalizuj plik(i) Pythona odpowiedzialne za moduÅ‚ "Baza Wiedzy".
2.  UtwÃ³rz nowÄ… funkcjÄ™, np. `def show_kmeans_docs():`.
3.  Wklej poniÅ¼szÄ… treÅ›Ä‡ do tej funkcji, uÅ¼ywajÄ…c `st.markdown()`.
4.  Dodaj w menu nawigacyjnym "Bazy Wiedzy" odnoÅ›nik prowadzÄ…cy do funkcji `show_kmeans_docs()`.

-----

## ğŸ“– TreÅ›Ä‡ dla moduÅ‚u "Baza Wiedzy"

(PoczÄ…tek treÅ›ci do skopiowania)

## ğŸ’  Klastrowanie K-Means (Grupowanie MetodÄ… K-Åšrednich)

K-Means to jeden z najpopularniejszych algorytmÃ³w **uczenia nienadzorowanego**. Jego celem jest automatyczne podzielenie zbioru danych na $K$ odrÄ™bnych grup (klastrÃ³w), gdzie punkty wewnÄ…trz jednego klastra sÄ… do siebie jak najbardziej podobne, a punkty miÄ™dzy rÃ³Å¼nymi klastrami â€“ jak najmniej.

### Jak DziaÅ‚a Algorytm K-Means?

Algorytm dziaÅ‚a iteracyjnie, prÃ³bujÄ…c znaleÅºÄ‡ "Å›rodki" (centroidy) dla $K$ grup.

1.  **Krok 1: WybÃ³r $K$**
    UÅ¼ytkownik musi na poczÄ…tku **zdecydowaÄ‡**, na ile grup ($K$) chce podzieliÄ‡ dane. (Patrz poniÅ¼ej, jak wybraÄ‡ $K$).
2.  **Krok 2: Inicjalizacja**
    Algorytm losowo (lub "inteligentnie" dziÄ™ki metodzie `k-means++`) umieszcza $K$ centroidÃ³w (punktÃ³w centralnych) w przestrzeni danych.
3.  **Krok 3: Przypisanie**
    KaÅ¼dy punkt danych jest przypisywany do **najbliÅ¼szego** mu centroidu (zazwyczaj na podstawie odlegÅ‚oÅ›ci euklidesowej).
4.  **Krok 4: Aktualizacja**
    Po przypisaniu wszystkich punktÃ³w, centroidy sÄ… **przesuwane**. NowÄ… lokalizacjÄ… kaÅ¼dego centroidu jest **Å›rednia arytmetyczna** wszystkich punktÃ³w, ktÃ³re zostaÅ‚y do niego przypisane.
5.  **Krok 5: PowtÃ³rzenie**
    Kroki 3 i 4 sÄ… powtarzane aÅ¼ do **konwergencji** â€“ czyli do momentu, gdy centroidy przestanÄ… siÄ™ znaczÄ…co przemieszczaÄ‡ (klastry siÄ™ ustabilizujÄ…).

-----

### Metody Statystyczne: "Jak wybraÄ‡ K?"

To najwiÄ™ksze wyzwanie w K-Means. Nie ma jednej "poprawnej" odpowiedzi, ale uÅ¼ywamy metod statystycznych, aby oszacowaÄ‡ dobrÄ… wartoÅ›Ä‡ $K$.

#### 1\. Metoda Åokcia (Elbow Method)

  * **Co mierzy:** **InercjÄ™ (WCSS)**, czyli sumÄ™ kwadratÃ³w odlegÅ‚oÅ›ci kaÅ¼dego punktu od jego centroidu. MÃ³wiÄ…c proÅ›ciej: jak bardzo "Å›cisÅ‚e" sÄ… klastry.
  * **Jak to dziaÅ‚a:** Uruchamiamy K-Means dla rÃ³Å¼nych wartoÅ›ci $K$ (np. od 2 do 10) i liczymy inercjÄ™ dla kaÅ¼dej z nich.
  * **Interpretacja:**
      * Im wiÄ™cej klastrÃ³w ($K$), tym mniejsza bÄ™dzie inercja (bo klastry sÄ… mniejsze i ciaÅ›niejsze).
      * Rysujemy wykres $K$ vs. Inercja.
      * Szukamy punktu **"zaÅ‚amania" (Å‚okcia)** â€“ miejsca, w ktÃ³rym linia przestaje gwaÅ‚townie opadaÄ‡. Jest to punkt, w ktÃ³rym dodanie kolejnego klastra nie przynosi juÅ¼ duÅ¼ej korzyÅ›ci (nie zmniejsza znaczÄ…co inercji).

#### 2\. Analiza Sylwetkowa (Silhouette Analysis)

  * **Co mierzy:** **WspÃ³Å‚czynnik Sylwetkowy (Silhouette Score)**. Jest to miara bardziej zaawansowana, ktÃ³ra ocenia dwie rzeczy jednoczeÅ›nie:
    1.  **SpÃ³jnoÅ›Ä‡ (Cohesion):** Jak blisko punkty sÄ… do innych punktÃ³w w tym samym klastrze?
    2.  **SeparacjÄ™ (Separation):** Jak daleko punkty sÄ… od punktÃ³w w *innych* klastrach?
  * **Jak to dziaÅ‚a:** Wynik jest obliczany dla kaÅ¼dego punktu i mieÅ›ci siÄ™ w zakresie od -1 do 1.
      * **+1:** Idealnie. Punkt jest daleko od sÄ…siednich klastrÃ³w i blisko swoich.
      * **0:** Punkt jest na granicy dwÃ³ch klastrÃ³w (nakÅ‚adanie siÄ™).
      * **-1:** Å¹le. Punkt jest prawdopodobnie przypisany do zÅ‚ego klastra.
  * **Interpretacja:**
      * Uruchamiamy K-Means dla rÃ³Å¼nych $K$ i liczymy *Å›redni* WspÃ³Å‚czynnik Sylwetkowy dla wszystkich punktÃ³w.
      * Rysujemy wykres $K$ vs. Åšredni Silhouette Score.
      * W przeciwieÅ„stwie do Metody Åokcia, tutaj **szukamy maksimum (szczytu)**. NajwyÅ¼szy wynik wskazuje na $K$, ktÃ³re daje najbardziej spÃ³jne i najlepiej odseparowane klastry.

-----

### Inne "Metody" i Wymagania K-Means

Aby K-Means zadziaÅ‚aÅ‚o poprawnie, nie wystarczy tylko wybraÄ‡ $K$. Kluczowe sÄ… teÅ¼ poniÅ¼sze "metody" (techniki).

#### 1\. Metoda: Standaryzacja Danych

  * **Problem:** Algorytm K-Means opiera siÄ™ na **odlegÅ‚oÅ›ci**. JeÅ›li masz cechy o rÃ³Å¼nych skalach (np. `Wiek` [0-100] i `Zarobki` [3 000 - 50 000]), cecha o wiÄ™kszej skali (`Zarobki`) caÅ‚kowicie zdominuje obliczenia odlegÅ‚oÅ›ci. Algorytm praktycznie zignoruje `Wiek`.
  * **RozwiÄ…zanie (Metoda):** **Standaryzacja** (np. `StandardScaler` w Scikit-learn).
  * **Co robi:** PrzeksztaÅ‚ca wszystkie cechy tak, aby miaÅ‚y Å›redniÄ… rÃ³wnÄ… 0 i odchylenie standardowe rÃ³wne 1. To sprawia, Å¼e kaÅ¼da cecha ma "rÃ³wnÄ… wagÄ™" w algorytmie.
  * **Wniosek:** **Prawie zawsze powinieneÅ› standaryzowaÄ‡ dane przed uÅ¼yciem K-Means.**

#### 2\. Metoda: Inicjalizacja (Problem Lokalne Minimum)

  * **Problem:** Wynik K-Means moÅ¼e zaleÅ¼eÄ‡ od tego, gdzie **na poczÄ…tku** zostaÅ‚y umieszczone centroidy (Krok 2). ZÅ‚y poczÄ…tkowy wybÃ³r moÅ¼e uwiÄ™ziÄ‡ algorytm w "lokalnym minimum" â€“ znajdzie on klastry, ale nie bÄ™dÄ… one optymalne.
  * **RozwiÄ…zanie (Metoda 1): `n_init`**
      * Uruchamiamy algorytm K-Means **wiele razy** (np. `n_init=10`) z rÃ³Å¼nymi losowymi punktami startowymi.
      * Jako ostateczny wynik wybierany jest ten przebieg, ktÃ³ry daÅ‚ najniÅ¼szÄ… inercjÄ™ (WCSS).
  * **RozwiÄ…zanie (Metoda 2): `init='k-means++'`**
      * To domyÅ›lna metoda inicjalizacji w Scikit-learn.
      * Zamiast umieszczaÄ‡ centroidy w peÅ‚ni losowo, `k-means++` robi to "inteligentnie" â€“ stara siÄ™ umieÅ›ciÄ‡ poczÄ…tkowe centroidy daleko od siebie. ZnaczÄ…co zwiÄ™ksza to szansÄ™ na znalezienie optymalnych klastrÃ³w i przyspiesza konwergencjÄ™.

(Koniec treÅ›ci do skopiowania)